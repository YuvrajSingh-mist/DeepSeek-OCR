Linear Independence and Dependence of Vectors

Given any set of \( m \) vectors \( \mathbf{a}_{(1)}, \ldots, \mathbf{a}_{(m)} \) (with the same number of components), a **linear combination** of these vectors is an expression of the form
\[
c_1 \mathbf{a}_{(1)}, + c_2 \mathbf{a}_{(2)}, + \ldots + c_m \mathbf{a}_{(m)}
\]
where \( c_1, c_2, \ldots, c_m \) are any scalars.
Now consider the equation
\[
\text{(1)} \qquad c_1 \mathbf{a}_{(1)}, + c_2 \mathbf{a}_{(2)}, + \ldots + c_m \mathbf{a}_{(m)} = 0
\]
Clearly, *this vector equation (1) holds if we choose all \( c_j \)'s zero, because then it becomes \( 0 = 0 \)*.
If this is the only \( m \)-tuple of scalars for which (1) holds, then our vectors \( \mathbf{a}_{(1)}, \ldots, \mathbf{a}_{(m)} \) are said to form a *linearly independent set* or, more briefly, we call them **linearly independent**.